{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJj9rZuJGcD8"
   },
   "source": [
    "# **Prepare Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOFkqu7d_ksQ",
    "outputId": "ed08b443-193d-4f1d-f812-4cf6649912d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check GPU\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CtkICc7Bdvyq",
    "outputId": "38037b13-b920-4f74-e692-36480b18de31"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in physical_devices:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6e89CSdMquYI"
   },
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pickle\n",
    "from pickle import dump, load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "from datetime import date, datetime, timedelta\n",
    "import time\n",
    "from pickle import load, dump\n",
    "import tensorflow as tf\n",
    "import talib\n",
    "import ta\n",
    "import itertools as itt\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import uniform\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from keras import regularizers\n",
    "from sklearn.tree import plot_tree\n",
    "from xgboost import plot_importance\n",
    "from lightgbm import plot_importance\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "# Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import lightgbm as lgb\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Conv2D, LSTM, GRU, Flatten, Dropout, MaxPooling2D, Reshape, Bidirectional\n",
    "from keras.layers import Conv1D, BatchNormalization, LeakyReLU, ELU, ReLU, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import LayerNormalization, MultiHeadAttention, Add, GlobalAveragePooling1D, GlobalAveragePooling2D\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Backtest library\n",
    "import vectorbt as vbt\n",
    "# import backtesting as bt\n",
    "# import pyfolio as pf\n",
    "# from backtesting import set_bokeh_output\n",
    "# set_bokeh_output(notebook=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTuag7l2KDGy"
   },
   "source": [
    "# **Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TqSJq5cQDTji"
   },
   "outputs": [],
   "source": [
    "# CPCV\n",
    "def cpcv_generator(t_span, num_groups, k):\n",
    "    # Split data into N groups, with N << t_span (i.e. N is significantly less than t_span)\n",
    "    # This will assign each index position to a group position\n",
    "    group_num = np.arange(t_span) // (t_span // num_groups)\n",
    "    group_num[group_num == num_groups] = num_groups-1\n",
    "\n",
    "    # Generate the combinations\n",
    "    test_groups = np.array(list(itt.combinations(np.arange(num_groups), k))).reshape(-1, k) # 15×2 matrix: [0,1],[0,2]...[0,5]...[3 5],[4 5]]\n",
    "    C_nk = len(test_groups)\n",
    "    n_paths = C_nk * k // num_groups\n",
    "\n",
    "    print('Number of simulations:', C_nk) # e.g. 15\n",
    "    print('Number of paths:', n_paths) # e.g. 5\n",
    "\n",
    "    # is_test is a T x C(n, k) array where each column is a logical array indicating which observation is in the test set\n",
    "    is_test_group = np.full((num_groups, C_nk), fill_value=False) # 6×15 matrix\n",
    "    is_test = np.full((t_span, C_nk), fill_value=False) # 100×15 matrix or 7566×15 matrix depending on T\n",
    "\n",
    "    # Assign test folds for each of the C(n, k) simulations\n",
    "    for sim_idx, pair in enumerate(test_groups):\n",
    "        i, j = pair # [0,1], [0,2],... [0,5], [1,2],..., [4,5]\n",
    "        is_test_group[[i, j], sim_idx] = True # is_test_group is 6×15 matrix.\n",
    "                                              # Since test_groups is [0,1], [0,2], [0,3], [0,4],... [0,5], [1,2],..., [4,5], simulation with idx 3 will consists of ticks in group 0 and 4 (i.e. the [0,4] pair)\n",
    "        # Assigning the test folds\n",
    "        mask = (group_num == i) | (group_num == j) # group_num = [0, 0, ..., 1, 1, ..., 2, 2, ..., 3, 3, ..., 4, 4, ...] of length 100 or 7566 (number of ticks)\n",
    "                                                   # for [i,j]=[0,4] mask becomes [True, True, ..., False, False, ..., False, False, ..., False, False, ..., True, True, ...]\n",
    "        is_test[mask, sim_idx] = True # Mark the rows that belong to group i,j so that they belong to sim_idx and are for testing and backtesting.\n",
    "                                      # For example, since groups [0,4] belong to simulation 3, we set simulation 3 to True for all the rows that belong to group 0 or 4 to indicate that they should be used\n",
    "                                      # for testing in simulation 3\n",
    "\n",
    "    # For each path, connect the folds from different simulations to form a test path.\n",
    "    # The fold coordinates are: the fold number, and the simulation index e.g. simulation 0, fold 0 etc\n",
    "    path_folds = np.full((num_groups, n_paths), fill_value=np.nan)\n",
    "    for p in range(n_paths):\n",
    "        for group in range(num_groups):\n",
    "            # The argmax() function explained: assuming is_test_group[group, :]=[F, F, F, T, F, F, F, T, F, F, T, F, T, F, T] then argmax() returns sim_idx=3\n",
    "            sim_idx = is_test_group[group, :].argmax().astype(int)\n",
    "            path_folds[group, p] = sim_idx  # Considering the above example where sim_idx=3 is associated with groups [0,4]:\n",
    "                                            #     path_folds[0,0], path_folds[0,1],..., path_folds[0,4] will be set to sim_idx=3\n",
    "                                            # Same for group 4:\n",
    "                                            #     path_folds[4,0], path_folds[4,1],..., path_folds[4,4] will be set to sim_idx=3\n",
    "\n",
    "            # Mark it as False so on the next iteration we get the next sim_idx when doing a \"...argmax().astype(int)\" (e.g. sim_idx=7)\n",
    "            is_test_group[group, sim_idx] = False\n",
    "\n",
    "\n",
    "    # Finally, for each path we indicate which simulation we're building the path from and the time indices\n",
    "    paths = np.full((t_span, n_paths), fill_value= np.nan) # 100×15 matrix or 7566×5 matrix\n",
    "    for p in range(n_paths):\n",
    "        for g in range(num_groups):\n",
    "            mask = (group_num == g) # Get all the ticks that belong to group g\n",
    "            paths[mask, p] = int(path_folds[g, p])\n",
    "\n",
    "    # Once done the matrices will look like so:\n",
    "    # is_test[99] = [F, F, F, F, True, F, F, F, True, F, F, True, F, True, True]\n",
    "    # paths[99] = [4, 8, 11, 13, 14]\n",
    "    # path_folds[5] = [4, 8, 11, 13, 14]\n",
    "    return (is_test, paths, path_folds)\n",
    "\n",
    "def purge(t1, test_times):\n",
    "    # Whatever is not in the train set should be in the test set\n",
    "    train = t1.copy(deep=True)\n",
    "    for test_start, test_end in test_times.items():\n",
    "        df_0 = train[(test_start <= train.index) & (train.index <= test_end)].index # train starts within test\n",
    "        df_1 = train[(test_start <= train) & (train <= test_end)].index # train ends within test\n",
    "        df_2 = train[(train.index <= test_start) & (test_end <= train)].index # train envelopes test\n",
    "        train = train.drop(df_0.union(df_1).union(df_2))\n",
    "    return train\n",
    "\n",
    "\n",
    "def embargo_(times, pct_embargo):\n",
    "    step = int(times.shape[0] * pct_embargo)\n",
    "    if step == 0:\n",
    "        ans = pd.Series(times, index=test_times)\n",
    "    else:\n",
    "        ans = pd.Series(times[step:].values, index=times[:-step].index)\n",
    "        ans = pd.concat([ans, pd.Series(times.iloc[-1], index=times[-step:].index)])\n",
    "    return ans\n",
    "\n",
    "\n",
    "def embargo(test_times, t1, pct_embargo=0.01):\n",
    "    t1_embargo = embargo_(t1, pct_embargo) # Embargoed t1\n",
    "    # Ensure the output contains only the relevant times that correspond to the indices of test_times.\n",
    "    test_times_embargoed = t1_embargo.loc[test_times.index]\n",
    "    return test_times_embargoed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "TjLFg82Y1cMP"
   },
   "outputs": [],
   "source": [
    "# --------------------- Function: Apply Sliding Window ---------------------\n",
    "def create_sliding_window(X, y, window_size=20):\n",
    "    X_windowed, y_windowed = [], []\n",
    "\n",
    "    for i in range(len(X) - window_size):\n",
    "        X_windowed.append(X[i : i + window_size])  # 20-day window\n",
    "        y_windowed.append(y[i + window_size])  # Target for next day\n",
    "\n",
    "    return np.array(X_windowed), np.array(y_windowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dKpXJdEX_bhQ"
   },
   "outputs": [],
   "source": [
    "# --------------------- Function: CPCV for ML models ---------------------\n",
    "def CPCV_ML(ticker, data, num_ticks, model, params, num_paths=6, num_groups_test=2):\n",
    "    t1_ = data.index\n",
    "    t1 = pd.Series(t1_, index=t1_) # t1 is both the trade time and the event time\n",
    "\n",
    "    # Realign t1\n",
    "    t1 = t1.loc[data.index]\n",
    "\n",
    "    num_groups = num_paths + 1\n",
    "    is_test, paths, _ = cpcv_generator(num_ticks, num_groups, num_groups_test)\n",
    "    pred = np.full(is_test.shape, np.nan)\n",
    "    # Store metrics and predictions\n",
    "    accuracies = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    mcc = []\n",
    "    auc = []\n",
    "\n",
    "    # CPCV Loop\n",
    "    num_sim = is_test.shape[1] # num of simulations needed to generate all backtest paths\n",
    "    for sim in tqdm(range(num_sim)):\n",
    "        # Get train set|\n",
    "        test_idx = is_test[:, sim]\n",
    "\n",
    "        # Convert numerical indices into time stamps\n",
    "        test_times = t1.loc[test_idx]\n",
    "\n",
    "        # Embargo\n",
    "        test_times_embargoed = embargo(test_times, t1, pct_embargo=0.01)\n",
    "\n",
    "        # Purge\n",
    "        train_times = purge(t1, test_times_embargoed)\n",
    "\n",
    "        # Split training / test sets\n",
    "        X_train = X.loc[train_times, :]\n",
    "        y_train = y.loc[train_times]\n",
    "\n",
    "        X_test = X.loc[test_times, :]\n",
    "        y_test = y.loc[test_times]\n",
    "\n",
    "        # Scaling\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Use RandomizedSearchCV to find best hyperparameters\n",
    "        base_classifier = model\n",
    "\n",
    "        random_search = RandomizedSearchCV(\n",
    "            base_classifier,\n",
    "            param_distributions=params,\n",
    "            n_iter=10,  # Number of random samples\n",
    "            cv=3,  # Inner cross-validation folds\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        random_search.fit(X_train, y_train)\n",
    "        classifier = random_search.best_estimator_  # Best model from search\n",
    "\n",
    "        pred_ = classifier.predict(X_test)\n",
    "        pred_ = pred_.reshape(-1, 1)\n",
    "        pred_ = np.concatenate(pred_, axis=0)\n",
    "\n",
    "        pred_ = (pred_ > 0.5)\n",
    "\n",
    "        # Metrics\n",
    "        accuracies.append(accuracy_score(y_test, pred_))\n",
    "        precision.append(precision_score(y_test, pred_))\n",
    "        recall.append(recall_score(y_test, pred_))\n",
    "        f1.append(f1_score(y_test, pred_))\n",
    "        mcc.append(matthews_corrcoef(y_test, pred_))\n",
    "        auc.append(roc_auc_score(y_test, pred_))\n",
    "\n",
    "        # Fill the backtesting prediction matrix\n",
    "        pred[test_idx, sim] = pred_\n",
    "    results = pd.DataFrame({\n",
    "    'Ticker': [ticker] * len(accuracies),\n",
    "    'Accuracy': accuracies,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1_Score': f1,\n",
    "    'MCC': mcc,\n",
    "    'AUC': auc\n",
    "    })\n",
    "    return results, pred, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "5ICI6NkXS_7y"
   },
   "outputs": [],
   "source": [
    "# --------------------- Function: CPCV for DL models ---------------------\n",
    "def CPCV_DL(ticker, data, num_ticks, model, num_paths=6, num_groups_test=2, time_steps=20, MLP_type=False):\n",
    "    # Define EarlyStopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', patience=3, restore_best_weights=True\n",
    "    )\n",
    "    if MLP_type==False:\n",
    "        t1_ = data[:-time_steps].index\n",
    "        t1 = pd.Series(t1_, index=t1_)  # t1 is both the trade time and event time\n",
    "        t1 = t1.loc[data[:-time_steps].index]  # Realign t1\n",
    "    else:\n",
    "        t1_ = data.index\n",
    "        t1 = pd.Series(t1_, index=t1_)  # t1 is both the trade time and event time\n",
    "        t1 = t1.loc[data.index]  # Realign t1\n",
    "    \n",
    "    num_groups = num_paths + 1\n",
    "    is_test, paths, _ = cpcv_generator(num_ticks, num_groups, num_groups_test)\n",
    "    pred = np.full(is_test.shape, np.nan)\n",
    "    # Store metrics and predictions\n",
    "    accuracies = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    mcc = []\n",
    "    auc = []\n",
    "\n",
    "    # CPCV Loop\n",
    "    num_sim = is_test.shape[1] # num of simulations needed to generate all backtest paths\n",
    "    for sim in tqdm(range(num_sim)):\n",
    "        # Get train set|\n",
    "        test_idx = is_test[:, sim]\n",
    "\n",
    "        # Convert numerical indices into time stamps\n",
    "        test_times = t1.loc[test_idx]\n",
    "\n",
    "        # Embargo\n",
    "        test_times_embargoed = embargo(test_times, t1, pct_embargo=0.01)\n",
    "\n",
    "        # Purge\n",
    "        train_times = purge(t1, test_times_embargoed)\n",
    "\n",
    "        if MLP_type==False:\n",
    "            # Split training/testing sets\n",
    "            X_train = X_windowed[train_times.index, :, :]\n",
    "            y_train = y_windowed[train_times.index]\n",
    "            X_test = X_windowed[test_times.index, :, :]\n",
    "            y_test = y_windowed[test_times.index]\n",
    "\n",
    "            # Standardization\n",
    "            scaler = StandardScaler()\n",
    "            X_train_shape = X_train.shape\n",
    "            X_test_shape = X_test.shape\n",
    "            X_train = scaler.fit_transform(X_train.reshape(-1, X_train_shape[2])).reshape(X_train_shape)\n",
    "            X_test = scaler.transform(X_test.reshape(-1, X_test_shape[2])).reshape(X_test_shape)\n",
    "        else:\n",
    "            # Split training/testing sets\n",
    "            X_train = X.loc[train_times, :]\n",
    "            y_train = y.loc[train_times]\n",
    "            X_test = X.loc[test_times, :]\n",
    "            y_test = y.loc[test_times]\n",
    "            # Scaling\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Use RandomizedSearchCV to find best hyperparameters\n",
    "        model = model\n",
    "\n",
    "        # Fit with early stopping\n",
    "        model.fit(X_train, y_train, batch_size=32, epochs=30, callbacks=[early_stopping], validation_split=0.2)\n",
    "\n",
    "        pred_ = (model.predict(X_test) > 0.5).astype(int).flatten()\n",
    "        pred[test_idx, sim] = pred_\n",
    "\n",
    "        # Metrics\n",
    "        accuracies.append(accuracy_score(y_test, pred_))\n",
    "        precision.append(precision_score(y_test, pred_))\n",
    "        recall.append(recall_score(y_test, pred_))\n",
    "        f1.append(f1_score(y_test, pred_))\n",
    "        mcc.append(matthews_corrcoef(y_test, pred_))\n",
    "        auc.append(roc_auc_score(y_test, pred_))\n",
    "\n",
    "        # Fill the backtesting prediction matrix\n",
    "        pred[test_idx, sim] = pred_\n",
    "    results = pd.DataFrame({\n",
    "    'Ticker': [ticker] * len(accuracies),\n",
    "    'Accuracy': accuracies,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1_Score': f1,\n",
    "    'MCC': mcc,\n",
    "    'AUC': auc\n",
    "    })\n",
    "    return results, pred, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ZLa61VxGGa43"
   },
   "outputs": [],
   "source": [
    "# --------------------- Function: Create backtest portfolio ---------------------\n",
    "def Backtest(data, pred, paths, time_steps=0, fixed_fees=0.001, slippage=0.001):\n",
    "    backtest_signals = pd.DataFrame(index=data['Date'][time_steps:], columns=[f'path_{p}' for p in range(paths.shape[1])])\n",
    "\n",
    "    # Iterate over backtest paths\n",
    "    for p in range(paths.shape[1]):\n",
    "        for t, sim in enumerate(paths[:, p]):  # index, value\n",
    "            backtest_signals.iloc[t, p] = pred[t, int(sim)]  # Keep signal values (1 or 0)\n",
    "\n",
    "    # Convert signals to numeric (ensuring they are 1s and 0s)\n",
    "    backtest_signals = backtest_signals.astype(float)\n",
    "\n",
    "    # Add close price column for vbt.Portfolio\n",
    "    backtest_signals['close'] = data['close'][time_steps:].values\n",
    "    backtest_signals['BnH'] = [1]*len(backtest_signals)\n",
    "    # Backtest using vectorbt\n",
    "    portfolio = vbt.Portfolio.from_signals(\n",
    "        close=backtest_signals['close'],\n",
    "        entries=backtest_signals.drop(['close'],axis=1) == 1,  # Entries where signal is 1\n",
    "        exits=backtest_signals.drop(['close'],axis=1) == 0,  # Exits where signal is 0\n",
    "        size=1,  # Assuming equal position size\n",
    "        fixed_fees=fixed_fees,  # Example transaction cost\n",
    "        slippage=slippage,\n",
    "        freq='1d'\n",
    "    )\n",
    "\n",
    "    return portfolio, backtest_signals\n",
    "\n",
    "# --------------------- Function: Plot backtest paths ---------------------\n",
    "def plot_backtest(portfolio, ticker, model_type):\n",
    "    cum_returns = (1 + portfolio.returns()).cumprod() - 1  # Compute cumulative return\n",
    "\n",
    "    # ✅ Plot multiple strategies manually\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for col in cum_returns.columns:\n",
    "        plt.plot(cum_returns.index, cum_returns[col], label=col)\n",
    "    mean_cum_return = cum_returns.mean(axis=1)\n",
    "    # Plot mean cumulative return (bold black line)\n",
    "    plt.plot(cum_returns.index, mean_cum_return, label=\"Mean Strategy\", linewidth=2, color=\"black\")\n",
    "    plt.legend()\n",
    "    plt.title(f'Cumulative Returns of {model_type} Strategies on {ticker}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Return')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# --------------------- Function: Trading metrics ---------------------\n",
    "def backtest_metrics(portfolio):\n",
    "    return portfolio.returns_stats()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- ML models ---------------------\n",
    "# LR\n",
    "LR = LogisticRegression(solver='liblinear')\n",
    "LR_params = {\n",
    "    'C': [0.1, 0.5, 1.0],  # Regularization strength\n",
    "    'penalty': ['l1', 'l2'],  # Type of regularization\n",
    "    'solver': ['liblinear'],  # Solver compatible with L1/L2 penalty\n",
    "    'class_weight': ['balanced', None]  # Class weights\n",
    "}\n",
    "\n",
    "# SVM\n",
    "SVM = SVC(probability=True)\n",
    "SVM_params = {\n",
    "    'C': [0.1, 0.5, 1.0],  # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Kernel types\n",
    "    'gamma': ['scale', 'auto'],  # Kernel coefficient\n",
    "}\n",
    "\n",
    "# DT\n",
    "DT = DecisionTreeClassifier()\n",
    "DT_params = {\n",
    "    'max_depth': [3, 5, 7],          # Fewer options for tree depth\n",
    "    'min_samples_split': [2, 3],     # Limited choices for splitting\n",
    "    'min_samples_leaf': [2, 3],      # Fewer options for leaf nodes\n",
    "    'criterion': ['gini', 'entropy'],   # Split criteria\n",
    "}\n",
    "\n",
    "# RF\n",
    "RF = RandomForestClassifier()\n",
    "RF_params = {\n",
    "    'n_estimators': [50, 100],  # Number of trees in the forest\n",
    "    'max_depth': [3, 5, 7],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 3],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2],  # Minimum number of samples required to be at a leaf node\n",
    "    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "# XGB\n",
    "XGB = XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')\n",
    "XGB_params = {\n",
    "    'n_estimators': [50, 100],         # Number of boosting rounds\n",
    "    'max_depth': [3, 5],                 # Depth of each tree\n",
    "    'learning_rate': [0.01, 0.05],     # Step size shrinkage\n",
    "    'subsample': [0.6, 0.8],           # Row sampling ratio\n",
    "    'colsample_bytree': [0.6, 0.8],    # Column sampling ratio per tree\n",
    "    'gamma': [0, 1],                     # Minimum loss reduction to split\n",
    "    'reg_alpha': [0, 0.1],               # L1 regularization term\n",
    "    'reg_lambda': [1, 2]               # L2 regularization term\n",
    "}\n",
    "\n",
    "# LGB\n",
    "LGB = lgb.LGBMClassifier(objective='binary', verbose=-1)\n",
    "LGB_params = {\n",
    "    'num_leaves': [7, 10],\n",
    "    'learning_rate': [0.01, 0.05],\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [3, 5],\n",
    "    'subsample': [0.6, 0.8],\n",
    "    'colsample_bytree': [0.6, 0.8],\n",
    "    'min_data_in_leaf': [1, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- DL models ---------------------\n",
    "\n",
    "# Define EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=3, restore_best_weights=True\n",
    ")\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def MLP_model(learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=X.shape[1]),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "    \n",
    "#------------------------------------------------------------------------\n",
    "def CNN_model(learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(20, X.shape[1], 1), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(units=128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def LSTM_model(learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        LSTM(units=64, return_sequences=True, input_shape=(20, X.shape[1]), kernel_regularizer=regularizers.l2(1e-3)),  # First LSTM layer\n",
    "        Dropout(0.2),\n",
    "        LSTM(units=128, return_sequences=True, kernel_regularizer=regularizers.l2(1e-3)),  # Second LSTM layer (stacked)\n",
    "        Dropout(0.2),\n",
    "        LSTM(units=128, return_sequences=False, kernel_regularizer=regularizers.l2(1e-3)),  # Third LSTM layer (final layer)\n",
    "        Dropout(0.2),\n",
    "        Dense(units=128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def GRU_model(learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        GRU(units=64, return_sequences=True, input_shape=(20, X.shape[1])),  # First GRU layer\n",
    "        Dropout(0.2),\n",
    "        GRU(units=128, return_sequences=True),  # Second GRU layer (stacked)\n",
    "        Dropout(0.2),\n",
    "        GRU(units=128, return_sequences=False),  # Third GRU layer (final layer)\n",
    "        Dropout(0.2),\n",
    "        Dense(units=128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------- Transformer Models ---------------------------------------------\n",
    "\n",
    "# Positional Encoding Layer\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, sequence_len, model_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.get_positional_encoding(sequence_len, model_dim)\n",
    "\n",
    "    def get_positional_encoding(self, seq_len, d_model):\n",
    "        pos = tf.range(seq_len, dtype=tf.float32)[:, tf.newaxis]\n",
    "        i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]\n",
    "        angle_rates = 1 / tf.pow(10000.0, (2 * (i//2)) / tf.cast(d_model, tf.float32))\n",
    "        angle_rads = pos * angle_rates\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        pos_encoding = tf.cast(self.pos_encoding[:, :tf.shape(x)[1], :], x.dtype)\n",
    "        return x + pos_encoding\n",
    "\n",
    "\n",
    "\n",
    "# Transformer Encoder Block\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.4):\n",
    "    # Multi-Head Self Attention\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    res = x + inputs  # Residual Connection\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = Dense(ff_dim, activation=\"gelu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    return x + res  # Residual Connection\n",
    "\n",
    "# Complete Transformer Model\n",
    "def Transformer_model(learning_rate=0.001, input_shape=(20, 105), head_size=32, num_heads=2, ff_dim=32, num_layers=2, dropout=0.4, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Positional Encoding\n",
    "    x = PositionalEncoding(input_shape[0], input_shape[1])(inputs)\n",
    "\n",
    "    # Transformer Blocks\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    # Output Layer for Classification\n",
    "    outputs = Dense(num_classes, activation=\"sigmoid\", kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------- Hybrid Models ---------------------------------------------\n",
    "def CNN_LSTM_model(input_shape=(20, 105, 1), learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "\n",
    "    # CNN feature extraction\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Replace Flatten with GlobalAveragePooling2D\n",
    "    model.add(Flatten())  \n",
    "\n",
    "    # Reshape for LSTM input (Ensure timesteps are valid)\n",
    "    model.add(Reshape((1, -1)))  # Single timestep\n",
    "\n",
    "    # LSTM sequence learning\n",
    "    model.add(LSTM(units=128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=128))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Layer Normalization\n",
    "    model.add(LayerNormalization())\n",
    "\n",
    "    # Fully connected layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def CNN_Transformer_model(input_shape=(20, 105), head_size=32, num_heads=2, ff_dim=32, num_layers=2, dropout=0.4, num_classes=1, learning_rate=0.001):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # CNN Feature Extraction\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2, strides=2, padding='same')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2, strides=2, padding='same')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv1D(128, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2, strides=2, padding='same')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # Positional Encoding\n",
    "    x = PositionalEncoding(x.shape[1], x.shape[2])(x)\n",
    "\n",
    "    # Transformer Blocks\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = Dense(num_classes, activation=\"sigmoid\", kernel_regularizer=regularizers.l2(1e-3))(x)\n",
    "\n",
    "    # Build Model\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_GAN(X_data, y_data, n_steps_in, n_steps_out):\n",
    "    X = list()\n",
    "    y = list()\n",
    "    yc = list()\n",
    "\n",
    "    length = len(X_data)\n",
    "    for i in range(0, length, 1):\n",
    "        X_value = X_data[i: i + n_steps_in][:, :]\n",
    "        y_value = y_data[i + n_steps_in: i + (n_steps_in + n_steps_out)][:, 0]\n",
    "        yc_value = y_data[i: i + n_steps_in][:, :]\n",
    "        if len(X_value) == n_steps_in and len(y_value) == n_steps_out:\n",
    "            X.append(X_value)\n",
    "            y.append(y_value)\n",
    "            yc.append(yc_value)\n",
    "\n",
    "    return np.array(X), np.array(y), np.array(yc)\n",
    "\n",
    "# RMSE\n",
    "def rmspe(y_true, y_pred):\n",
    "    # Calculate Mean Squared Error\n",
    "    mse = np.mean(np.square(y_true - y_pred))\n",
    "    # Calculate RMSPE\n",
    "    return np.sqrt(mse) / np.mean(y_true)\n",
    "\n",
    "# Define the Generator\n",
    "def Generator(input_dim, output_dim, feature_size) -> tf.keras.models.Model:\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=128,\n",
    "                  return_sequences=True,\n",
    "                  input_shape=(input_dim, feature_size),\n",
    "                   kernel_regularizer=regularizers.l2(1e-3)\n",
    "                  ))\n",
    "    model.add(LSTM(units=128,\n",
    "                  return_sequences=True,\n",
    "                   kernel_regularizer=regularizers.l2(1e-3)\n",
    "                  ))\n",
    "    model.add(LSTM(units=64,\n",
    "                   kernel_regularizer=regularizers.l2(1e-3)\n",
    "                  ))\n",
    "    model.add(Dense(128,\n",
    "                  kernel_regularizer=regularizers.l2(1e-3)))\n",
    "    model.add(Dense(64, kernel_regularizer=regularizers.l2(1e-3)))\n",
    "    model.add(Dense(32, kernel_regularizer=regularizers.l2(1e-3)))\n",
    "    model.add(Dense(16, kernel_regularizer=regularizers.l2(1e-3)))\n",
    "    model.add(Dense(8, kernel_regularizer=regularizers.l2(1e-3)))\n",
    "    model.add(Dense(units=output_dim, dtype=tf.float32))\n",
    "    return model\n",
    "\n",
    "# Define the discriminator\n",
    "def Discriminator(n_steps_in) -> tf.keras.models.Model:\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv1D(32, input_shape=(n_steps_in+1, 1), kernel_size=3, strides=2, padding=\"same\", activation=LeakyReLU(alpha=0.01)))\n",
    "    model.add(Conv1D(64, kernel_size=3, strides=2, padding=\"same\", activation=LeakyReLU(alpha=0.01)))\n",
    "    model.add(Conv1D(128, kernel_size=3, strides=2, padding=\"same\", activation=LeakyReLU(alpha=0.01)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(220, use_bias=True))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dense(220, use_bias=True))\n",
    "    model.add(ReLU())\n",
    "    model.add(Dense(1, dtype=tf.float32))\n",
    "    return model\n",
    "\n",
    "# Train WGAN-GP model\n",
    "class GAN():\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(GAN, self).__init__()\n",
    "        self.d_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "        self.g_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.batch_size = 128\n",
    "        checkpoint_dir = '../training_checkpoints'\n",
    "        self.checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "        self.checkpoint = tf.train.Checkpoint(generator_optimizer=self.g_optimizer,\n",
    "                                              discriminator_optimizer=self.d_optimizer,\n",
    "                                              generator=self.generator,\n",
    "                                              discriminator=self.discriminator)\n",
    "\n",
    "        # Save best model\n",
    "        self.best_model_path = 'best_gen_GRU_model.keras'\n",
    "\n",
    "        # For tracking best validation loss\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_output, fake_output):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # get the interpolated data\n",
    "        alpha = tf.random.normal([batch_size, 21, 1], 0.0, 1.0)\n",
    "        diff = fake_output - tf.cast(real_output, tf.float32)\n",
    "        interpolated = tf.cast(real_output, tf.float32) + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "\n",
    "        # 3. Calcuate the norm of the gradients\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
    "\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, data):\n",
    "        real_input, real_price, yc = data\n",
    "        batch_size = tf.shape(real_input)[0]\n",
    "        for _ in range(1):\n",
    "            with tf.GradientTape() as d_tape:\n",
    "                # Train the discriminator\n",
    "                # generate fake output\n",
    "                generated_data = self.generator(real_input, training=True)\n",
    "                # reshape the data\n",
    "                generated_data_reshape = tf.reshape(generated_data, [generated_data.shape[0], generated_data.shape[1], 1])\n",
    "                fake_output = tf.concat([generated_data_reshape, tf.cast(yc, tf.float32)], axis=1)\n",
    "                real_y_reshape = tf.reshape(real_price, [real_price.shape[0], real_price.shape[1], 1])\n",
    "                real_output = tf.concat([tf.cast(real_y_reshape, tf.float32), tf.cast(yc, tf.float32)], axis=1)\n",
    "                # Get the logits for the fake images\n",
    "                D_real = self.discriminator(real_output, training=True)\n",
    "                # Get the logits for real images\n",
    "                D_fake = self.discriminator(fake_output, training=True)\n",
    "                # Calculate discriminator loss using fake and real logits\n",
    "                real_loss = tf.cast(tf.reduce_mean(D_real), tf.float32)\n",
    "                fake_loss = tf.cast(tf.reduce_mean(D_fake), tf.float32)\n",
    "                d_cost = fake_loss-real_loss\n",
    "                # Calculate the gradientjiu penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_output, fake_output)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * 10\n",
    "\n",
    "            d_grads = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            self.d_optimizer.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))\n",
    "        for _ in range(3):\n",
    "            with tf.GradientTape() as g_tape:\n",
    "                # Train the generator\n",
    "                # generate fake output\n",
    "                generated_data = self.generator(real_input, training=True)\n",
    "                # reshape the data\n",
    "                generated_data_reshape = tf.reshape(generated_data, [generated_data.shape[0], generated_data.shape[1], 1])\n",
    "                fake_output = tf.concat([generated_data_reshape, tf.cast(yc, tf.float32)], axis=1)\n",
    "                # Get the discriminator logits for fake images\n",
    "                G_fake = self.discriminator(fake_output, training=True)\n",
    "                # Calculate the generator loss\n",
    "                g_loss = -tf.reduce_mean(G_fake)\n",
    "            g_grads = g_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "            self.g_optimizer.apply_gradients(zip(g_grads, self.generator.trainable_variables))\n",
    "\n",
    "        return real_price, generated_data, {'d_loss': d_loss, 'g_loss': g_loss}\n",
    "\n",
    "    def train(self, X_train, y_train, yc, epochs):\n",
    "        data = X_train, y_train, yc\n",
    "        train_hist = {}\n",
    "        train_hist['D_losses'] = []\n",
    "        train_hist['G_losses'] = []\n",
    "        train_hist['per_epoch_times'] = []\n",
    "        train_hist['total_ptime'] = []\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "\n",
    "            real_price, fake_price, loss = self.train_step(data)\n",
    "\n",
    "            G_losses = []\n",
    "            D_losses = []\n",
    "\n",
    "            Real_price = []\n",
    "            Predicted_price = []\n",
    "\n",
    "            D_losses.append(loss['d_loss'].numpy())\n",
    "            G_losses.append(loss['g_loss'].numpy())\n",
    "\n",
    "            Predicted_price.append(fake_price)\n",
    "            Real_price.append(real_price)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            val_pred = self.generator(X_train, training=False)\n",
    "            val_loss = tf.reduce_mean(tf.keras.losses.MSE(y_train, val_pred))\n",
    "\n",
    "            # Save best model\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                tf.keras.models.save_model(generator, self.best_model_path)\n",
    "                # print(f\"Best model saved at epoch {epoch+1} with val_loss: {val_loss.numpy()}\")\n",
    "\n",
    "            # # Save the model every 15 epochs\n",
    "            # if (epoch + 1) % 15 == 0:\n",
    "            #     tf.keras.models.save_model(generator, 'gen_GRU_model_%d.h5' % epoch)\n",
    "            #     tf.keras.models.save_model(discriminator, 'dis_CNN_model%d.h5' % epoch)\n",
    "            #     self.checkpoint.save(file_prefix=self.checkpoint_prefix)\n",
    "            #     print('epoch', epoch+1, 'd_loss', loss['d_loss'].numpy(), 'g_loss', loss['g_loss'].numpy())\n",
    "\n",
    "            # For printing loss\n",
    "            epoch_end_time = time.time()\n",
    "            per_epoch_ptime = epoch_end_time - start\n",
    "            train_hist['D_losses'].append(D_losses)\n",
    "            train_hist['G_losses'].append(G_losses)\n",
    "            train_hist['per_epoch_times'].append(per_epoch_ptime)\n",
    "\n",
    "        # Reshape the predicted result & real\n",
    "        Predicted_price = np.array(Predicted_price)\n",
    "        Predicted_price = Predicted_price.reshape(Predicted_price.shape[1], Predicted_price.shape[2])\n",
    "        Real_price = np.array(Real_price)\n",
    "        Real_price = Real_price.reshape(Real_price.shape[1], Real_price.shape[2])\n",
    "\n",
    "        return Predicted_price, Real_price, rmspe(Predicted_price,Real_price)\n",
    "\n",
    "    def predict(self, X_test, training = False):\n",
    "        # Load the best saved model\n",
    "        G_model = tf.keras.models.load_model(self.best_model_path)\n",
    "\n",
    "        # Recompile the model (only needed if training again)\n",
    "        # G_model.compile(\n",
    "        #     optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        #     loss=tf.keras.losses.BinaryCrossentropy(),  # Use your appropriate loss function\n",
    "        #     metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()]  # Add relevant metrics\n",
    "        # )\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = G_model.predict(X_test)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontier = [ 'VNINDEX', 'BET']\n",
    "for ticker in frontier:\n",
    "    # Set data\n",
    "    data, pd.read_csv(f'{ticker}.csv')\n",
    "    X = data.drop(['Target_20', 'Return_20d', 'Date'], axis=1)\n",
    "    y = data[\"Target_20\"]\n",
    "    X_windowed, y_windowed = create_sliding_window(X.values, y.values, window_size=20)\n",
    "    time_steps = 20\n",
    "    num_ticks_ML = len(X)\n",
    "    num_ticks_DL = len(X_windowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>Return_20d</th>\n",
       "      <th>Target_20</th>\n",
       "      <th>angle of 3 comp</th>\n",
       "      <th>...</th>\n",
       "      <th>DONCHIAN_high</th>\n",
       "      <th>DONCHIAN_low</th>\n",
       "      <th>KeltnerChannel_high</th>\n",
       "      <th>KeltnerChannel_low</th>\n",
       "      <th>UlcerIndex</th>\n",
       "      <th>EMV</th>\n",
       "      <th>FI</th>\n",
       "      <th>NVI</th>\n",
       "      <th>VPT</th>\n",
       "      <th>VWAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72</td>\n",
       "      <td>2000-04-14</td>\n",
       "      <td>545.63</td>\n",
       "      <td>545.63</td>\n",
       "      <td>532.99</td>\n",
       "      <td>535.52</td>\n",
       "      <td>1111000.0</td>\n",
       "      <td>-0.052060</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.202457</td>\n",
       "      <td>...</td>\n",
       "      <td>567.48</td>\n",
       "      <td>567.48</td>\n",
       "      <td>551.785167</td>\n",
       "      <td>551.785167</td>\n",
       "      <td>4.525545</td>\n",
       "      <td>-7867.290729</td>\n",
       "      <td>-1.854630e+06</td>\n",
       "      <td>1152.854711</td>\n",
       "      <td>4.002629e+04</td>\n",
       "      <td>540.484728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>2000-04-17</td>\n",
       "      <td>535.52</td>\n",
       "      <td>542.24</td>\n",
       "      <td>531.72</td>\n",
       "      <td>542.24</td>\n",
       "      <td>734330.0</td>\n",
       "      <td>-0.012673</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.202837</td>\n",
       "      <td>...</td>\n",
       "      <td>554.49</td>\n",
       "      <td>554.49</td>\n",
       "      <td>550.434833</td>\n",
       "      <td>550.434833</td>\n",
       "      <td>4.264800</td>\n",
       "      <td>-3337.954326</td>\n",
       "      <td>-8.847264e+05</td>\n",
       "      <td>1167.321367</td>\n",
       "      <td>4.924107e+04</td>\n",
       "      <td>539.459908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74</td>\n",
       "      <td>2000-04-18</td>\n",
       "      <td>542.24</td>\n",
       "      <td>542.31</td>\n",
       "      <td>536.40</td>\n",
       "      <td>540.99</td>\n",
       "      <td>825810.0</td>\n",
       "      <td>-0.001255</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.203216</td>\n",
       "      <td>...</td>\n",
       "      <td>554.49</td>\n",
       "      <td>554.49</td>\n",
       "      <td>549.962500</td>\n",
       "      <td>549.962500</td>\n",
       "      <td>3.973216</td>\n",
       "      <td>1699.694845</td>\n",
       "      <td>-9.058030e+05</td>\n",
       "      <td>1167.321367</td>\n",
       "      <td>4.733737e+04</td>\n",
       "      <td>539.172934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75</td>\n",
       "      <td>2000-04-19</td>\n",
       "      <td>540.99</td>\n",
       "      <td>540.99</td>\n",
       "      <td>522.66</td>\n",
       "      <td>529.39</td>\n",
       "      <td>1206000.0</td>\n",
       "      <td>-0.018430</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.203595</td>\n",
       "      <td>...</td>\n",
       "      <td>554.49</td>\n",
       "      <td>554.49</td>\n",
       "      <td>550.145500</td>\n",
       "      <td>550.145500</td>\n",
       "      <td>3.743217</td>\n",
       "      <td>-11444.850746</td>\n",
       "      <td>-2.774917e+06</td>\n",
       "      <td>1167.321367</td>\n",
       "      <td>2.147811e+04</td>\n",
       "      <td>536.886903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76</td>\n",
       "      <td>2000-04-20</td>\n",
       "      <td>529.39</td>\n",
       "      <td>529.39</td>\n",
       "      <td>513.03</td>\n",
       "      <td>513.44</td>\n",
       "      <td>1552000.0</td>\n",
       "      <td>-0.052134</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.203973</td>\n",
       "      <td>...</td>\n",
       "      <td>554.49</td>\n",
       "      <td>554.49</td>\n",
       "      <td>549.539167</td>\n",
       "      <td>549.539167</td>\n",
       "      <td>3.504071</td>\n",
       "      <td>-11189.523196</td>\n",
       "      <td>-5.914843e+06</td>\n",
       "      <td>1167.321367</td>\n",
       "      <td>-2.528212e+04</td>\n",
       "      <td>532.303028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6169</th>\n",
       "      <td>6241</td>\n",
       "      <td>2024-12-20</td>\n",
       "      <td>16424.98</td>\n",
       "      <td>16860.58</td>\n",
       "      <td>16424.98</td>\n",
       "      <td>16728.81</td>\n",
       "      <td>20528000.0</td>\n",
       "      <td>-0.023679</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.171837</td>\n",
       "      <td>...</td>\n",
       "      <td>17456.33</td>\n",
       "      <td>17456.33</td>\n",
       "      <td>17174.009833</td>\n",
       "      <td>17174.009833</td>\n",
       "      <td>4.460573</td>\n",
       "      <td>285162.246687</td>\n",
       "      <td>-8.665359e+08</td>\n",
       "      <td>3472.079334</td>\n",
       "      <td>-2.916983e+06</td>\n",
       "      <td>16640.104456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6170</th>\n",
       "      <td>6242</td>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>16728.81</td>\n",
       "      <td>17045.38</td>\n",
       "      <td>16728.81</td>\n",
       "      <td>16857.73</td>\n",
       "      <td>28667000.0</td>\n",
       "      <td>-0.007338</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.172248</td>\n",
       "      <td>...</td>\n",
       "      <td>17456.33</td>\n",
       "      <td>17456.33</td>\n",
       "      <td>17168.398667</td>\n",
       "      <td>17168.398667</td>\n",
       "      <td>3.836248</td>\n",
       "      <td>269797.326368</td>\n",
       "      <td>-2.147809e+08</td>\n",
       "      <td>3472.079334</td>\n",
       "      <td>-2.696062e+06</td>\n",
       "      <td>16703.979048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6171</th>\n",
       "      <td>6243</td>\n",
       "      <td>2024-12-27</td>\n",
       "      <td>16857.73</td>\n",
       "      <td>17044.91</td>\n",
       "      <td>16857.36</td>\n",
       "      <td>16924.44</td>\n",
       "      <td>13227000.0</td>\n",
       "      <td>-0.003603</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.172659</td>\n",
       "      <td>...</td>\n",
       "      <td>17456.33</td>\n",
       "      <td>17456.33</td>\n",
       "      <td>17169.907000</td>\n",
       "      <td>17169.907000</td>\n",
       "      <td>3.345973</td>\n",
       "      <td>90804.430332</td>\n",
       "      <td>-5.804457e+07</td>\n",
       "      <td>3485.819168</td>\n",
       "      <td>-2.643720e+06</td>\n",
       "      <td>16815.996948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6172</th>\n",
       "      <td>6244</td>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>16924.44</td>\n",
       "      <td>16924.58</td>\n",
       "      <td>16657.58</td>\n",
       "      <td>16720.75</td>\n",
       "      <td>23093000.0</td>\n",
       "      <td>-0.014778</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.173070</td>\n",
       "      <td>...</td>\n",
       "      <td>17456.33</td>\n",
       "      <td>17456.33</td>\n",
       "      <td>17166.451333</td>\n",
       "      <td>17166.451333</td>\n",
       "      <td>2.947602</td>\n",
       "      <td>-185054.713550</td>\n",
       "      <td>-7.217258e+08</td>\n",
       "      <td>3485.819168</td>\n",
       "      <td>-2.921650e+06</td>\n",
       "      <td>16871.433123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6173</th>\n",
       "      <td>6245</td>\n",
       "      <td>2025-01-03</td>\n",
       "      <td>16720.75</td>\n",
       "      <td>17043.95</td>\n",
       "      <td>16718.24</td>\n",
       "      <td>17031.70</td>\n",
       "      <td>9578000.0</td>\n",
       "      <td>0.022110</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.173480</td>\n",
       "      <td>...</td>\n",
       "      <td>17456.33</td>\n",
       "      <td>17456.33</td>\n",
       "      <td>17174.402333</td>\n",
       "      <td>17174.402333</td>\n",
       "      <td>2.837132</td>\n",
       "      <td>306105.508979</td>\n",
       "      <td>-1.931537e+08</td>\n",
       "      <td>3550.643741</td>\n",
       "      <td>-2.743531e+06</td>\n",
       "      <td>16938.418949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6174 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        Date      open      high       low     close  \\\n",
       "0             72  2000-04-14    545.63    545.63    532.99    535.52   \n",
       "1             73  2000-04-17    535.52    542.24    531.72    542.24   \n",
       "2             74  2000-04-18    542.24    542.31    536.40    540.99   \n",
       "3             75  2000-04-19    540.99    540.99    522.66    529.39   \n",
       "4             76  2000-04-20    529.39    529.39    513.03    513.44   \n",
       "...          ...         ...       ...       ...       ...       ...   \n",
       "6169        6241  2024-12-20  16424.98  16860.58  16424.98  16728.81   \n",
       "6170        6242  2024-12-23  16728.81  17045.38  16728.81  16857.73   \n",
       "6171        6243  2024-12-27  16857.73  17044.91  16857.36  16924.44   \n",
       "6172        6244  2024-12-30  16924.44  16924.58  16657.58  16720.75   \n",
       "6173        6245  2025-01-03  16720.75  17043.95  16718.24  17031.70   \n",
       "\n",
       "          volume  Return_20d  Target_20  angle of 3 comp  ...  DONCHIAN_high  \\\n",
       "0      1111000.0   -0.052060          0        -0.202457  ...         567.48   \n",
       "1       734330.0   -0.012673          0        -0.202837  ...         554.49   \n",
       "2       825810.0   -0.001255          0        -0.203216  ...         554.49   \n",
       "3      1206000.0   -0.018430          0        -0.203595  ...         554.49   \n",
       "4      1552000.0   -0.052134          0        -0.203973  ...         554.49   \n",
       "...          ...         ...        ...              ...  ...            ...   \n",
       "6169  20528000.0   -0.023679          0        -0.171837  ...       17456.33   \n",
       "6170  28667000.0   -0.007338          0        -0.172248  ...       17456.33   \n",
       "6171  13227000.0   -0.003603          0        -0.172659  ...       17456.33   \n",
       "6172  23093000.0   -0.014778          0        -0.173070  ...       17456.33   \n",
       "6173   9578000.0    0.022110          1        -0.173480  ...       17456.33   \n",
       "\n",
       "      DONCHIAN_low  KeltnerChannel_high  KeltnerChannel_low  UlcerIndex  \\\n",
       "0           567.48           551.785167          551.785167    4.525545   \n",
       "1           554.49           550.434833          550.434833    4.264800   \n",
       "2           554.49           549.962500          549.962500    3.973216   \n",
       "3           554.49           550.145500          550.145500    3.743217   \n",
       "4           554.49           549.539167          549.539167    3.504071   \n",
       "...            ...                  ...                 ...         ...   \n",
       "6169      17456.33         17174.009833        17174.009833    4.460573   \n",
       "6170      17456.33         17168.398667        17168.398667    3.836248   \n",
       "6171      17456.33         17169.907000        17169.907000    3.345973   \n",
       "6172      17456.33         17166.451333        17166.451333    2.947602   \n",
       "6173      17456.33         17174.402333        17174.402333    2.837132   \n",
       "\n",
       "                EMV            FI          NVI           VPT          VWAP  \n",
       "0      -7867.290729 -1.854630e+06  1152.854711  4.002629e+04    540.484728  \n",
       "1      -3337.954326 -8.847264e+05  1167.321367  4.924107e+04    539.459908  \n",
       "2       1699.694845 -9.058030e+05  1167.321367  4.733737e+04    539.172934  \n",
       "3     -11444.850746 -2.774917e+06  1167.321367  2.147811e+04    536.886903  \n",
       "4     -11189.523196 -5.914843e+06  1167.321367 -2.528212e+04    532.303028  \n",
       "...             ...           ...          ...           ...           ...  \n",
       "6169  285162.246687 -8.665359e+08  3472.079334 -2.916983e+06  16640.104456  \n",
       "6170  269797.326368 -2.147809e+08  3472.079334 -2.696062e+06  16703.979048  \n",
       "6171   90804.430332 -5.804457e+07  3485.819168 -2.643720e+06  16815.996948  \n",
       "6172 -185054.713550 -7.217258e+08  3485.819168 -2.921650e+06  16871.433123  \n",
       "6173  306105.508979 -1.931537e+08  3550.643741 -2.743531e+06  16938.418949  \n",
       "\n",
       "[6174 rows x 109 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9jSeDL8KGU7"
   },
   "source": [
    "# **Work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of simulations: 21\n",
      "Number of paths: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2be8d14079b44529b13e8f46b3a758c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 20, 105), found shape=(None, 20, 106)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# ---------------------------------------- DL models ---------------------------------------------\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Transformer\u001b[39;00m\n\u001b[0;32m     15\u001b[0m Transformer_model \u001b[38;5;241m=\u001b[39m Transformer_model(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m Transformer_result_df, Transformer_pred, Transformer_paths \u001b[38;5;241m=\u001b[39m \u001b[43mCPCV_DL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_ticks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_ticks_DL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTransformer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                                                                     \u001b[49m\u001b[43mnum_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMLP_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m Transformer_result_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Transformer.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m Transformer_portfolio, Transformer_signal \u001b[38;5;241m=\u001b[39m Backtest(data, Transformer_pred, Transformer_paths,time_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 70\u001b[0m, in \u001b[0;36mCPCV_DL\u001b[1;34m(ticker, data, num_ticks, model, num_paths, num_groups_test, time_steps, MLP_type)\u001b[0m\n\u001b[0;32m     67\u001b[0m model \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Fit with early stopping\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m pred_ \u001b[38;5;241m=\u001b[39m (model\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     73\u001b[0m pred[test_idx, sim] \u001b[38;5;241m=\u001b[39m pred_\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\2\\__autograph_generated_file1tgu6u3l.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Administrator\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 20, 105), found shape=(None, 20, 106)\n"
     ]
    }
   ],
   "source": [
    "frontier = [ 'VNINDEX', 'BET']\n",
    "for ticker in frontier:\n",
    "    # Set data\n",
    "    data, pd.read_csv(f'{ticker}.csv')\n",
    "    X = data.drop(['Target_20', 'Return_20d', 'Date'], axis=1)\n",
    "    y = data[\"Target_20\"]\n",
    "    X_windowed, y_windowed = create_sliding_window(X.values, y.values, window_size=20)\n",
    "    time_steps = 20\n",
    "    num_ticks_ML = len(X)\n",
    "    num_ticks_DL = len(X_windowed)\n",
    "    \n",
    "        # ---------------------------------------- DL models ---------------------------------------------\n",
    "    \n",
    "        # Transformer\n",
    "    Transformer_model = Transformer_model(learning_rate=0.0005)\n",
    "    Transformer_result_df, Transformer_pred, Transformer_paths = CPCV_DL(ticker=ticker, data=data, num_ticks=num_ticks_DL, model=Transformer_model, \n",
    "                                                                         num_paths=6, num_groups_test=2, time_steps=20, MLP_type=False)\n",
    "    Transformer_result_df.to_csv(f'{ticker}_Transformer.csv', index=False)\n",
    "    Transformer_portfolio, Transformer_signal = Backtest(data, Transformer_pred, Transformer_paths,time_steps=20)\n",
    "        \n",
    "        # MLP\n",
    "    MLP_model = MLP_model(learning_rate=0.0005)\n",
    "    MLP_result_df, MLP_pred, MLP_paths = CPCV_DL(ticker=ticker, data=data, num_ticks=num_ticks_ML, model=MLP_model, \n",
    "                                                 num_paths=6, num_groups_test=2, time_steps=20, MLP_type=True)\n",
    "    MLP_result_df.to_csv(f'{ticker}_MLP.csv', index=False)\n",
    "    MLP_portfolio, MLP_signal = Backtest(data, MLP_pred, MLP_paths,time_steps=0)\n",
    "    \n",
    "        # CNN\n",
    "    CNN_model = CNN_model(learning_rate=0.0005)\n",
    "    CNN_result_df, CNN_pred, CNN_paths = CPCV_DL(ticker=ticker, data=data, num_ticks=num_ticks_DL, model=CNN_model, \n",
    "                                                 num_paths=6, num_groups_test=2, time_steps=20, MLP_type=False)\n",
    "    CNN_result_df.to_csv(f'{ticker}_CNN.csv', index=False)\n",
    "    CNN_portfolio, CNN_signal = Backtest(data, CNN_pred, CNN_paths,time_steps=20)\n",
    "        \n",
    "        # LSTM\n",
    "    LSTM_model = LSTM_model(learning_rate=0.0005)\n",
    "    LSTM_result_df, LSTM_pred, LSTM_paths = CPCV_DL(ticker=ticker, data=data, num_ticks=num_ticks_DL, model=LSTM_model, \n",
    "                                                    num_paths=6, num_groups_test=2, time_steps=20, MLP_type=False)\n",
    "    LSTM_result_df.to_csv(f'{ticker}_LSTM.csv', index=False)\n",
    "    LSTM_portfolio, LSTM_signal = Backtest(data, LSTM_pred, LSTM_paths,time_steps=20)\n",
    "        \n",
    "        # GRU\n",
    "    GRU_model = GRU_model(learning_rate=0.0005)\n",
    "    GRU_result_df, GRU_pred, GRU_paths = CPCV_DL(ticker=ticker, data=data, num_ticks=num_ticks_DL, model=GRU_model, \n",
    "                                                 num_paths=6, num_groups_test=2, time_steps=20, MLP_type=False)\n",
    "    GRU_result_df.to_csv(f'{ticker}_GRU.csv', index=False)\n",
    "    GRU_portfolio, GRU_signal = Backtest(data, GRU_pred, GRU_paths,time_steps=20)\n",
    "        \n",
    "        # CNN-LSTM\n",
    "    CNN_LSTM_model = CNN_LSTM_model(learning_rate=0.0005)\n",
    "    CNN_LSTM_result_df, CNN_LSTM_pred, CNN_LSTM_paths = CPCV_DL(ticker=ticker, data=data, num_ticks=num_ticks_DL, model=CNN_LSTM_model, \n",
    "                                                                num_paths=6, num_groups_test=2, time_steps=20, MLP_type=False)\n",
    "    CNN_LSTM_result_df.to_csv(f'{ticker}_CNN_LSTM.csv', index=False)\n",
    "    CNN_LSTM_portfolio, CNN_LSTM_signal = Backtest(data, CNN_LSTM_pred, CNN_LSTM_paths,time_steps=20)\n",
    "        \n",
    "        # CNN-Transformer\n",
    "    CNN_Transformer_model = CNN_Transformer_model(learning_rate=0.0005)\n",
    "    CNN_Transformer_result_df, CNN_Transformer_pred, CNN_Transformer_paths = CPCV_DL(ticker=ticker, data=data, num_ticks=num_ticks_DL, model=CNN_Transformer_model, \n",
    "                                                                                     num_paths=6, num_groups_test=2, time_steps=20, MLP_type=False)\n",
    "    CNN_Transformer_result_df.to_csv(f'{ticker}_CNN_Transformer.csv', index=False)\n",
    "    CNN_Transformer_portfolio, CNN_Transformer_signal = Backtest(data, CNN_Transformer_pred, CNN_Transformer_paths,time_steps=20)\n",
    "    \n",
    "        # ---------------------- GAN -------------------------------------\n",
    "    X_value = pd.DataFrame(X)\n",
    "    y_value = pd.DataFrame(data['Return_20d'])\n",
    "        \n",
    "    n_steps_in = 20\n",
    "    n_features = X_value.shape[1]\n",
    "    n_steps_out = 1\n",
    "        \n",
    "        # Scale the data\n",
    "    X_scaler = StandardScaler()\n",
    "    y_scaler = StandardScaler()\n",
    "        \n",
    "    X_scale_dataset = X_scaler.fit_transform(X_value)\n",
    "    y_scale_dataset = y_scaler.fit_transform(y_value)\n",
    "        \n",
    "        # Process data\n",
    "    X_gan, y_gan, yc = get_X_y_GAN(X_scale_dataset, y_scale_dataset, n_steps_in, n_steps_out)\n",
    "    \n",
    "        # --------------------- CPCV & Data Processing ---------------------\n",
    "    t1_ = data[:-n_steps_in].index\n",
    "    t1 = pd.Series(t1_, index=t1_)  # t1 is both the trade time and event time\n",
    "    t1 = t1.loc[data[:-n_steps_in].index]  # Realign t1\n",
    "        \n",
    "    num_paths = 6\n",
    "    num_groups_test = 2\n",
    "    num_groups = num_paths + 1\n",
    "    num_ticks = len(X_gan)\n",
    "    is_test, paths, _ = cpcv_generator(num_ticks, num_groups, num_groups_test)\n",
    "        \n",
    "    pred = np.full(is_test.shape, np.nan)\n",
    "        \n",
    "        # Store metrics and predictions\n",
    "    all_accuracies = []\n",
    "    all_precision = []\n",
    "    all_recall = []\n",
    "    all_f1 = []\n",
    "    all_mcc = []\n",
    "    all_auc = []\n",
    "        \n",
    "        # --------------------- CPCV Loop ---------------------\n",
    "    num_sim = is_test.shape[1]  # Number of CPCV simulations\n",
    "        \n",
    "    for sim in tqdm(range(num_sim)):\n",
    "            # Get train/test indices\n",
    "            test_idx = is_test[:, sim]\n",
    "            test_times = t1.loc[test_idx]  # Select test times\n",
    "        \n",
    "            # Update `test_idx` to reflect the new `test_times`\n",
    "            test_idx = np.where(is_test[:, sim])[0]\n",
    "        \n",
    "            # Embargo & Purge\n",
    "            test_times_embargoed = embargo(test_times, t1, pct_embargo=0.01)\n",
    "            train_times = purge(t1, test_times_embargoed)\n",
    "        \n",
    "            # Split training/testing sets\n",
    "            X_train_gan = X_gan[train_times.index, :, :]\n",
    "            y_train_gan = y_gan[train_times.index]\n",
    "            yc_train = yc[train_times.index]\n",
    "        \n",
    "            X_test_gan = X_gan[test_times.index, :, :]\n",
    "            y_test_gan = y_gan[test_times.index]\n",
    "            yc_test = yc[test_times.index]\n",
    "            y_test = y[test_times.index]\n",
    "        \n",
    "            # Train WGAN model\n",
    "            input_dim = X_train_gan.shape[1]\n",
    "            feature_size = X_train_gan.shape[2]\n",
    "            output_dim = y_train_gan.shape[1]\n",
    "            epoch = 50\n",
    "        \n",
    "            generator = Generator(X_train_gan.shape[1], output_dim, X_train_gan.shape[2])\n",
    "            discriminator = Discriminator(n_steps_in)\n",
    "            gan = GAN(generator, discriminator)\n",
    "            Predicted_price, Real_price, RMSPE = gan.train(X_train_gan, y_train_gan, yc_train, epochs=50)\n",
    "        \n",
    "            # Predictions\n",
    "            y_pred = gan.predict(X_test_gan)\n",
    "            rescaled_predicted_y = y_scaler.inverse_transform(y_pred)\n",
    "        \n",
    "            # Combine predictions and real values into a single DataFrame\n",
    "            y_predict = (y_pred.flatten()> 0.0).astype(int)\n",
    "            pred[test_idx, sim] = (y_predict)\n",
    "        \n",
    "            # Store Metrics\n",
    "            all_accuracies.append(accuracy_score(y_test, y_predict))\n",
    "            all_precision.append(precision_score(y_test, y_predict))\n",
    "            all_recall.append(recall_score(y_test, y_predict))\n",
    "            all_f1.append(f1_score(y_test, y_predict))\n",
    "            all_mcc.append(matthews_corrcoef(y_test, y_predict))\n",
    "            all_auc.append(roc_auc_score(y_test, y_predict))\n",
    "    GAN_result_df = pd.DataFrame({\n",
    "        'Ticker': [ticker] * len(all_accuracies),\n",
    "        'Accuracy': all_accuracies,\n",
    "        'Precision': all_precision,\n",
    "        'Recall': all_recall,\n",
    "        'F1_Score': all_f1,\n",
    "        'MCC': all_mcc,\n",
    "        'AUC': all_auc\n",
    "        })\n",
    "    \n",
    "    GAN_pred = pred\n",
    "    GAN_paths = paths\n",
    "    GAN_result_df.to_csv(f'{ticker}_GAN.csv', index=False)\n",
    "    GAN_portfolio, GAN_signal = Backtest(data, GAN_pred, GAN_paths,time_steps=20)\n",
    "        \n",
    "    \n",
    "        # ---------------------------------------- ML models ---------------------------------------------\n",
    "        # LR model\n",
    "    LR_result_df, LR_pred, LR_paths = CPCV_ML(ticker=ticker, data=data, num_ticks=num_ticks_ML, \n",
    "                                                      model=LR, params=LR_params, num_paths=6, num_groups_test=2)\n",
    "    LR_result_df.to_csv(f'{ticker}_LR.csv', index=False)\n",
    "    LR_portfolio, LR_signal = Backtest(data, LR_pred, LR_paths,time_steps=0)\n",
    "        \n",
    "        # SVM model\n",
    "    SVM_result_df, SVM_pred, SVM_paths = CPCV_ML(ticker=ticker, data=data, num_ticks=num_ticks_ML, \n",
    "                                                      model=SVM, params=SVM_params, num_paths=6, num_groups_test=2)\n",
    "    SVM_result_df.to_csv(f'{ticker}_SVM.csv', index=False)\n",
    "    SVM_portfolio, SVM_signal = Backtest(data, SVM_pred, SVM_paths,time_steps=0)\n",
    "        \n",
    "        # DT model\n",
    "    DT_result_df, DT_pred, DT_paths = CPCV_ML(ticker=ticker, data=data, num_ticks=num_ticks_ML, \n",
    "                                                      model=DT, params=DT_params, num_paths=6, num_groups_test=2)\n",
    "    DT_result_df.to_csv(f'{ticker}_DT.csv', index=False)\n",
    "    DT_portfolio, DT_signal = Backtest(data, DT_pred, DT_paths,time_steps=0)\n",
    "        \n",
    "        # RF model\n",
    "    RF_result_df, RF_pred, RF_paths = CPCV_ML(ticker=ticker, data=data, num_ticks=num_ticks_ML, \n",
    "                                                      model=RF, params=RF_params, num_paths=6, num_groups_test=2)\n",
    "    RF_result_df.to_csv(f'{ticker}_RF.csv', index=False)\n",
    "    RF_portfolio, RF_signal = Backtest(data, RF_pred, RF_paths,time_steps=0)\n",
    "        \n",
    "        # XGB model\n",
    "    XGB_result_df, XGB_pred, XGB_paths = CPCV_ML(ticker=ticker, data=data, num_ticks=num_ticks_ML, \n",
    "                                                      model=XGB, params=XGB_params, num_paths=6, num_groups_test=2)\n",
    "    XGB_result_df.to_csv(f'{ticker}_XGB.csv', index=False)\n",
    "    XGB_portfolio, XGB_signal = Backtest(data, XGB_pred, XGB_paths,time_steps=0)\n",
    "        \n",
    "        # LGB model\n",
    "    LGB_result_df, LGB_pred, LGB_paths = CPCV_ML(ticker=ticker, data=data, num_ticks=num_ticks_ML, \n",
    "                                                      model=LGB, params=LGB_params, num_paths=6, num_groups_test=2)\n",
    "    LGB_result_df.to_csv(f'{ticker}_LGB.csv', index=False)\n",
    "    LGB_portfolio, LGB_signal = Backtest(data, LGB_pred, LGB_paths,time_steps=0)\n",
    "    \n",
    "       \n",
    "    \n",
    "        # -------------- Store results ------------\n",
    "    # List of model names\n",
    "    models = [\"LR\", \"SVM\", \"DT\", \"RF\", \"XGB\", \"LGB\", \"MLP\", \"CNN\", \"LSTM\", \"GRU\", \"Transformer\", \"CNN_LSTM\", \"CNN_Transformer\", \"GAN\"]\n",
    "    \n",
    "    # Compute returns\n",
    "    returns = {model: (globals()[f\"{model}_portfolio\"].returns()) * (globals()[f\"{model}_signal\"].drop(\"close\", axis=1)) for model in models}\n",
    "    \n",
    "    # Create index_return DataFrame\n",
    "    index_return = pd.DataFrame(index=returns[\"LR\"].index)\n",
    "    index_return[\"BnH\"] = returns[\"LR\"][\"BnH\"]\n",
    "    for model in models:\n",
    "        index_return[model] = returns[model].drop(\"BnH\", axis=1).mean(axis=1)\n",
    "    index_return.to_csv(f\"{ticker}_return.csv\", index=False)\n",
    "    \n",
    "    # Compute cumulative returns\n",
    "    cum_returns = {model: (1 + globals()[f\"{model}_portfolio\"].returns()).cumprod() - 1 for model in models}\n",
    "    mean_cum_returns = {model: cum_returns[model].mean(axis=1) for model in models}\n",
    "    \n",
    "    # Compute BnH cumulative return\n",
    "    BnH_cum_return = (1 + index_return[\"BnH\"]).cumprod() - 1\n",
    "    \n",
    "    # Align missing data using forward fill\n",
    "    for model in models:\n",
    "        mean_cum_returns[model] = mean_cum_returns[model].reindex(mean_cum_returns[\"LR\"].index, method=\"ffill\")\n",
    "    \n",
    "    # Create a DataFrame to store performance metrics\n",
    "    metrics_df = pd.DataFrame()\n",
    "    \n",
    "    # Collect performance metrics from each portfolio\n",
    "    for model in models:\n",
    "        portfolio = globals()[f\"{model}_portfolio\"]\n",
    "        stats = portfolio.returns_stats()\n",
    "        metrics_df[model] = stats\n",
    "    \n",
    "    # Save metrics as CSV\n",
    "    metrics_df.to_csv(f\"{ticker}_trading.csv\")\n",
    "    print(\"Saved model performance metrics to model_performance_metrics.csv\")\n",
    "    \n",
    "    # Print the table for quick view\n",
    "    print(metrics_df)\n",
    "\n",
    "    # Plot cumulative returns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for model in models:\n",
    "        plt.plot(mean_cum_returns[model].index, mean_cum_returns[model], label=f\"{model} Mean Strategy\")\n",
    "    \n",
    "    # Highlight BnH line\n",
    "    plt.plot(BnH_cum_return.index, BnH_cum_return, label=\"BnH\", linewidth=3, color='black', linestyle='dashed')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(f\"Cumulative Returns of ML and DL Strategies in {ticker}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Cumulative Return\")\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"{ticker}.png\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "035fcf55eadb4a3fa72e31c870e665fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b9272f870d14cd1bdbe65ff2daeaced",
      "placeholder": "​",
      "style": "IPY_MODEL_c5b0ebd71a364958a2b4d0c655c8cb79",
      "value": " 10/10 [05:59&lt;00:00, 35.00s/it]"
     }
    },
    "2b7905423c24457e98b82eef8d77332b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4856b632c7c4e08b92a7c3d50a37285",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f3fd08aec7b94536b0eeb57e24baeebf",
      "value": 10
     }
    },
    "3a128621989e44a29a2c39da1f2fffd0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44490bf74c7c45789f811ea9807a4d78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_91a31e57fee340109cef595a98a6895f",
       "IPY_MODEL_2b7905423c24457e98b82eef8d77332b",
       "IPY_MODEL_b92b1933523f45bbb8e8ea038b505c33"
      ],
      "layout": "IPY_MODEL_530fb52e6ff14e59a682b3726b51105c"
     }
    },
    "46e91c4340cb4d4ba2371a948e9e8570": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a62748b152745b4b1965f5d1409e456",
       "IPY_MODEL_d0b4a6fc6dea43539ecfae30b6986651",
       "IPY_MODEL_035fcf55eadb4a3fa72e31c870e665fe"
      ],
      "layout": "IPY_MODEL_3a128621989e44a29a2c39da1f2fffd0"
     }
    },
    "49aba4ae62814149a70991e5f3f6a9c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "530fb52e6ff14e59a682b3726b51105c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65d1eb47850c4343960b04475ab31bac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6a62748b152745b4b1965f5d1409e456": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4cfae20de3748d8b07ad90f3c30095b",
      "placeholder": "​",
      "style": "IPY_MODEL_f606a44fdde3435abf4bedd0040fcdfa",
      "value": "100%"
     }
    },
    "8b9272f870d14cd1bdbe65ff2daeaced": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91a31e57fee340109cef595a98a6895f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49aba4ae62814149a70991e5f3f6a9c5",
      "placeholder": "​",
      "style": "IPY_MODEL_65d1eb47850c4343960b04475ab31bac",
      "value": "100%"
     }
    },
    "a2bc548b06b34c79a0c19aed183f4fea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b4cfae20de3748d8b07ad90f3c30095b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4cfb5af9b2a4689bbeedaf6748f9293": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b92b1933523f45bbb8e8ea038b505c33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb79789a27304d6993fb209096fe6603",
      "placeholder": "​",
      "style": "IPY_MODEL_a2bc548b06b34c79a0c19aed183f4fea",
      "value": " 10/10 [03:59&lt;00:00, 25.90s/it]"
     }
    },
    "bb79789a27304d6993fb209096fe6603": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd7000b9af3d44aaa661ba5e8b502255": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c5b0ebd71a364958a2b4d0c655c8cb79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0b4a6fc6dea43539ecfae30b6986651": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4cfb5af9b2a4689bbeedaf6748f9293",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bd7000b9af3d44aaa661ba5e8b502255",
      "value": 10
     }
    },
    "e4856b632c7c4e08b92a7c3d50a37285": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3fd08aec7b94536b0eeb57e24baeebf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f606a44fdde3435abf4bedd0040fcdfa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
